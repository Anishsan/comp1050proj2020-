{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is in this file:\n",
    "# Cell 2: the imports that would need to be included\n",
    "# Cell 3: an existing code block provided for you to create training/test dataset (feature dataset)\n",
    "# Cell 4: Challenge 1\n",
    "# Cell 5: Challenge 2\n",
    "# Cell 6: an existing code block provided for you to  train the machine learning model, test the machine learning model, and see the results\n",
    "# Cell 7: Challenge 3\n",
    "# Cell 8: Challenge 4\n",
    "# Cell 9: Submission preparation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"def create_feature_data_from_files(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    feature_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_samples = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_samples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sample_number in range(number_of_samples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sample_data = activity_data[ \n",
    "                                1000 * sample_number : 1000 * (sample_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_sample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_sample.append(int(sample_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_sample = np.array([feature_sample]) \n",
    "                feature_set = np.concatenate((feature_set, feature_sample), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_feature = pd.DataFrame(feature_set)\n",
    "    df_feature.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create '+ output_filename +' ... check if the file was created!')\n",
    "    print(str(len(feature_set)) + \" data rows should be in the output file\")\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_testing_data_from_files(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    testing_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_training_samples = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_training_samples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sample_number in range(number_of_training_samples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sample_data = activity_data[ \n",
    "                                1000 * sample_number : 1000 * (sample_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_sample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_sample.append(int(sample_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_sample = np.array([feature_sample]) \n",
    "                testing_set = np.concatenate((testing_set, feature_sample), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_testing = pd.DataFrame(testing_set)\n",
    "    df_testing.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create '+ output_filename +' ... check if the file was created!')\n",
    "    print(str(len(testing_set)) + \" data rows should be in the output file\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files being used for testing data[11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File dataset_11.txt does not exist: 'dataset_11.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-eb91be2bf7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#provide your code below for tutors to check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcreate_testing_data_from_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenamesToUseForTestingData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'week12_testing_data_9Participants.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-354309631cc6>\u001b[0m in \u001b[0;36mcreate_testing_data_from_files\u001b[0;34m(list_of_filenames, output_filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#import the file contents into a panadas data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimported_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#generate \"features\" for each activitiy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File dataset_11.txt does not exist: 'dataset_11.txt'"
     ]
    }
   ],
   "source": [
    "#Challenge 1:\n",
    "#you need to create a testing data set based on dataset_11,12,13,14,15,16,17,18,and 19\n",
    "#the testing dataset name is called week12_testing_data_9Participants.csv\n",
    "\n",
    "filenamesToUseForTestingData = []\n",
    "datasetFileNumbers = [11,12,13,14,15,16,17,18,19]\n",
    "\n",
    "print(\"Dataset files being used for testing data\"+ str(datasetFileNumbers))\n",
    "#\n",
    "# Start: create testing data\n",
    "#\n",
    "for number in datasetFileNumbers:\n",
    "    filenamesToUseForTestingData.append(\"dataset_\" + str(number)+\".txt\")\n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "create_testing_data_from_files(filenamesToUseForTestingData, 'week12_testing_data_9Participants.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 2:\n",
    "#you need to create a training data set based on dataset_1,2,3,4,5,6,7,8,9 and 10\n",
    "#the testing dataset name is called week12_training_data_10Participants.csv\n",
    "\n",
    "#\n",
    "# Start: create training data\n",
    "#\n",
    "\n",
    "#\n",
    "# End: create training data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def model_training_and_evaluation(traing_file_name, testing_file_name):\n",
    "    df_training = pd.read_csv(traing_file_name, header=None)\n",
    "    df_testing = pd.read_csv(testing_file_name, header=None)\n",
    "\n",
    "    training_labels = df_training[9].values\n",
    "    # Labels should start from 0 in sklearn\n",
    "    training_labels = training_labels - 1\n",
    "    df_training = df_training.drop([9], axis=1)\n",
    "    training_features = df_training.values\n",
    "\n",
    "    testing_labels = df_testing[9].values\n",
    "    testing_labels = testing_labels - 1\n",
    "    df_testing = df_testing.drop([9], axis=1)\n",
    "    testing_features = df_testing.values\n",
    "\n",
    "    # Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "    # StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "    scaler = preprocessing.StandardScaler().fit(training_features)\n",
    "    training_features = scaler.transform(training_features)\n",
    "    testing_features = scaler.transform(testing_features)\n",
    "\n",
    "    # Build KNN classifier, in this example code\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(training_features, training_labels)\n",
    "\n",
    "    # Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "    # We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy. In this example\n",
    "    # code, when n_neighbors is set to 4, the accuracy achieves 0.757.\n",
    "    predicted_labels = knn.predict(testing_features)\n",
    "    print('Accuracy: ', accuracy_score(testing_labels, predicted_labels))\n",
    "    # We could use confusion matrix to view the classification for each activity.\n",
    "    print(confusion_matrix(testing_labels, predicted_labels))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Challenge 3\n",
    "#use created 10 participants training data and 9 participants testing data to train and test the KNN model\n",
    "#And interpret the results\n",
    "\n",
    "# Begining your code here\n",
    "\n",
    "\n",
    "# End\n",
    "    \n",
    "    \n",
    "# Activity values: 1 – Sitting, 2 – Lying down, 3 – Standing, 4 – Washing Dishes, 5 – Vacuuming, \n",
    "# 6 – Sweeping, 7 – Walking outside, 8 – Ascending stairs, 9 – Descending stairs, 10 – Treadmill running, 11 – Bicycling, 12 – Bicycling (more intense), 13 – Rope Jumping.\n",
    "\n",
    "\n",
    "# Can you interpret the confusion matrix below?\n",
    "# what are the rows and what are the columns?\n",
    "# which activity the machine learning predicts best?\n",
    "\n",
    "#write your answers below for tutors to check\n",
    "#Rows are for true labels\n",
    "#Columns are for predicted labels \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 4\n",
    "# Let's try another way for training and testing \n",
    "# Hint: training data and testing data shall not overlap \n",
    "# Solution: use odd number for training, use even number for testing\n",
    "# odd number means dataset_1, 3,5.....19, even number means dataset_2,4,6....18\n",
    "\n",
    "\n",
    "#begin your code here\n",
    "\n",
    "#end code\n",
    "\n",
    "# Activity values: 1 – Sitting, 2 – Lying down, 3 – Standing, 4 – Washing Dishes, 5 – Vacuuming, \n",
    "# 6 – Sweeping, 7 – Walking outside, 8 – Ascending stairs, 9 – Descending stairs, 10 – Treadmill running, 11 – Bicycling, 12 – Bicycling (more intense), 13 – Rope Jumping.\n",
    "\n",
    "\n",
    "# Can you interpret the confusion matrix above?\n",
    "# what are the rows and what are the columns?\n",
    "# which activity the machine learning predicts best?\n",
    "\n",
    "# Has the result been improved?  why?\n",
    "\n",
    "# How to improve?\n",
    "\n",
    "#write your answers below for tutors to check\n",
    "#Rows are for true labels\n",
    "#Columns are for predicted labels \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare your submission\n",
    "\n",
    "# 1. prepare your code submission. if one file, name it historyCode(.ipynb), if multiple file, zip them and named it \n",
    "# historyCode.zip \n",
    "\n",
    "# 2. rename this python code file as main(.ipynb)\n",
    "\n",
    "# 3. Take a screenshot of your github commit history, name it githubCommit(.png/jpg/gif)\n",
    "\n",
    "# 4. Take a screenshot of your Agile task assignment (in Trello or whatever tools you used), name it agileTask.(.png/jpg/gif)\n",
    "\n",
    "# 5. Submit your workbook as myWorkBook.pdf (if you don't record your logbook electronically, you need to take photos of your handwritten ones and create a PDF from these photos). \n",
    "\n",
    "# 6. call the tutor to check whether your submission files are correct and if correct, submit now (all the files mentioned above).\n",
    "\n",
    "# 7. Ask the tutor what is your group's order in week 13's demo session\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
