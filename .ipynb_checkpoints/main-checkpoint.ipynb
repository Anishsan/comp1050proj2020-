{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is in this file:\n",
    "# Cell 2: the imports that would need to be included\n",
    "# Cell 3: an existing code block provided for you to create training/test dataset (feature dataset)\n",
    "# Cell 4: Challenge 1\n",
    "# Cell 5: Challenge 2\n",
    "# Cell 6: an existing code block provided for you to  train the machine learning model, test the machine learning model, and see the results\n",
    "# Cell 7: Challenge 3\n",
    "# Cell 8: Challenge 4\n",
    "# Cell 9: Submission preparation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_feature_data_from_files(list_of_filenames, output_filename):\n",
    "    \n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    feature_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_samples = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_samples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sample_number in range(number_of_samples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sample_data = activity_data[ \n",
    "                                1000 * sample_number : 1000 * (sample_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_sample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_sample.append(int(sample_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_sample = np.array([feature_sample]) \n",
    "                feature_set = np.concatenate((feature_set, feature_sample), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_feature = pd.DataFrame(feature_set)\n",
    "    df_feature.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create '+ output_filename +' ... check if the file was created!')\n",
    "    print(str(len(feature_set)) + \" data rows should be in the output file\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingDataFromFiles(listOfFilenames, outputFilename):\n",
    "    \"\"\"\n",
    "    This function takes list of files and then creates test data and outputs it to the appropriate .csv file\n",
    "    \"\"\"\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    testingSet = np.empty(shape =(0, 10))\n",
    "    \n",
    "    for datasetFile in listOfFilenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        importedData = pd.read_csv(datasetFile, sep =',', header = None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activityData = importedData[importedData[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog = False)\n",
    "            for j in range(24):\n",
    "                activityData[:, j] = signal.lfilter(b, a, activityData[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            numberOfTrainingSamples = int( len(activityData)/1000 )\n",
    "            print(  \"File \" + datasetFile +\n",
    "                    \" has \" + str(numberOfTrainingSamples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sampleNumber in range(numberOfTrainingSamples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sampleData = activityData[ \n",
    "                                1000 * sampleNumber : 1000 * (sampleNumber + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                featureSample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    featureSample.append(np.min(sampleData[:, i]))\n",
    "                    featureSample.append(np.max(sampleData[:, i]))\n",
    "                    featureSample.append(np.mean(sampleData[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                featureSample.append(int(sampleData[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                featureSample = np.array([featureSample]) \n",
    "                testingSet = np.concatenate((testingSet, featureSample), axis = 0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    dfTesting = pd.DataFrame(testingSet)\n",
    "    dfTesting.to_csv(outputFilename, index = None, header = None)\n",
    "    print('attempted to create '+ outputFilename +' ... check if the file was created!')\n",
    "    print(str(len(testingSet)) + \" data rows should be in the output file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingDataFromFiles(listOfFilenames, outputFilename):\n",
    "    \"\"\"\n",
    "    This function takes list of files and then creates train data and outputs it to the appropriate .csv file\n",
    "    \"\"\"\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    trainingSet = np.empty(shape = (0, 10))\n",
    "    \n",
    "    for datasetFile in listOfFilenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        importedData = pd.read_csv(datasetFile, sep = ',', header = None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activityData = importedData[importedData[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog = False)\n",
    "            for j in range(24):\n",
    "                activityData[:, j] = signal.lfilter(b, a, activityData[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            numberOfTrainingSamples = int( len(activityData)/1000 )\n",
    "            print(  \"File \" + datasetFile +\n",
    "                    \" has \" + str(numberOfTrainingSamples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sampleNumber in range(numberOfTrainingSamples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sampleData = activityData[ \n",
    "                                1000 * sampleNumber : 1000 * (sampleNumber + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                featureSample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    featureSample.append(np.min(sampleData[:, i]))\n",
    "                    featureSample.append(np.max(sampleData[:, i]))\n",
    "                    featureSample.append(np.mean(sampleData[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                featureSample.append(int(sampleData[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                featureSample = np.array([featureSample]) \n",
    "                trainingSet = np.concatenate((trainingSet, featureSample), axis = 0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    dfTraining = pd.DataFrame(trainingSet)\n",
    "    dfTraining.to_csv(outputFilename, index =  None, header = None)\n",
    "    print('attempted to create '+ outputFilename +' ... check if the file was created!')\n",
    "    print(str(len(trainingSet)) + \" data rows should be in the output file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files being used for testing data[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_5.txt has 13 samples of 1000 rowsfor activity: 5\n",
      "File dataset_5.txt has 19 samples of 1000 rowsfor activity: 6\n",
      "File dataset_5.txt has 66 samples of 1000 rowsfor activity: 7\n",
      "File dataset_5.txt has 6 samples of 1000 rowsfor activity: 8\n",
      "File dataset_5.txt has 5 samples of 1000 rowsfor activity: 9\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_5.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_6.txt has 14 samples of 1000 rowsfor activity: 6\n",
      "File dataset_6.txt has 32 samples of 1000 rowsfor activity: 7\n",
      "File dataset_6.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_6.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_6.txt has 23 samples of 1000 rowsfor activity: 12\n",
      "File dataset_6.txt has 13 samples of 1000 rowsfor activity: 13\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_7.txt has 13 samples of 1000 rowsfor activity: 6\n",
      "File dataset_7.txt has 78 samples of 1000 rowsfor activity: 7\n",
      "File dataset_7.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_7.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_7.txt has 4 samples of 1000 rowsfor activity: 13\n",
      "File dataset_8.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 2\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_8.txt has 26 samples of 1000 rowsfor activity: 4\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_8.txt has 22 samples of 1000 rowsfor activity: 6\n",
      "File dataset_8.txt has 41 samples of 1000 rowsfor activity: 7\n",
      "File dataset_8.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_8.txt has 5 samples of 1000 rowsfor activity: 9\n",
      "File dataset_8.txt has 23 samples of 1000 rowsfor activity: 10\n",
      "File dataset_8.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_8.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_8.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 6\n",
      "File dataset_9.txt has 53 samples of 1000 rowsfor activity: 7\n",
      "File dataset_9.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_9.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_9.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_9.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_9.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_10.txt has 16 samples of 1000 rowsfor activity: 6\n",
      "File dataset_10.txt has 49 samples of 1000 rowsfor activity: 7\n",
      "File dataset_10.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_10.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_10.txt has 11 samples of 1000 rowsfor activity: 13\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_11.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_11.txt has 17 samples of 1000 rowsfor activity: 6\n",
      "File dataset_11.txt has 55 samples of 1000 rowsfor activity: 7\n",
      "File dataset_11.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_11.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_11.txt has 23 samples of 1000 rowsfor activity: 10\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_11.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_12.txt has 13 samples of 1000 rowsfor activity: 1\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_12.txt has 28 samples of 1000 rowsfor activity: 4\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_12.txt has 28 samples of 1000 rowsfor activity: 6\n",
      "File dataset_12.txt has 48 samples of 1000 rowsfor activity: 7\n",
      "File dataset_12.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_12.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_12.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_13.txt has 16 samples of 1000 rowsfor activity: 6\n",
      "File dataset_13.txt has 59 samples of 1000 rowsfor activity: 7\n",
      "File dataset_13.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_13.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_13.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_13.txt has 5 samples of 1000 rowsfor activity: 13\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_14.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_14.txt has 58 samples of 1000 rowsfor activity: 7\n",
      "File dataset_14.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_14.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_14.txt has 5 samples of 1000 rowsfor activity: 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset_15.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_15.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_15.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_15.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_15.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_15.txt has 62 samples of 1000 rowsfor activity: 7\n",
      "File dataset_15.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_15.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_15.txt has 23 samples of 1000 rowsfor activity: 12\n",
      "File dataset_15.txt has 1 samples of 1000 rowsfor activity: 13\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_16.txt has 32 samples of 1000 rowsfor activity: 4\n",
      "File dataset_16.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_16.txt has 18 samples of 1000 rowsfor activity: 6\n",
      "File dataset_16.txt has 60 samples of 1000 rowsfor activity: 7\n",
      "File dataset_16.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_16.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_16.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_16.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_16.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_16.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_17.txt has 27 samples of 1000 rowsfor activity: 6\n",
      "File dataset_17.txt has 44 samples of 1000 rowsfor activity: 7\n",
      "File dataset_17.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_17.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_17.txt has 27 samples of 1000 rowsfor activity: 12\n",
      "File dataset_17.txt has 0 samples of 1000 rowsfor activity: 13\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_18.txt has 23 samples of 1000 rowsfor activity: 6\n",
      "File dataset_18.txt has 36 samples of 1000 rowsfor activity: 7\n",
      "File dataset_18.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_18.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_18.txt has 3 samples of 1000 rowsfor activity: 13\n",
      "File dataset_19.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_19.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_19.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_19.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_19.txt has 17 samples of 1000 rowsfor activity: 6\n",
      "File dataset_19.txt has 40 samples of 1000 rowsfor activity: 7\n",
      "File dataset_19.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_19.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_19.txt has 10 samples of 1000 rowsfor activity: 13\n",
      "attempted to create TestingData7Participants.csv ... check if the file was created!\n",
      "3547 data rows should be in the output file\n"
     ]
    }
   ],
   "source": [
    "#Challenge 1:\n",
    "#you need to create a testing data set based on dataset_11,12,13,14,15,16,17,18,and 19\n",
    "#the testing dataset name is called week12_testing_data_9Participants.csv\n",
    "\n",
    "filenamesToUseForTestingData = []\n",
    "datasetFileNumbers = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]\n",
    "\n",
    "print(\"Dataset files being used for testing data\"+ str(datasetFileNumbers))\n",
    "#\n",
    "# Start: create testing data\n",
    "#\n",
    "for number in datasetFileNumbers:\n",
    "    filenamesToUseForTestingData.append(\"dataset_\" + str(number)+\".txt\")\n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "createTestingDataFromFiles(filenamesToUseForTestingData, 'TestingData7Participants.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files being used for testing data[1, 2, 3, 4]\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_1.txt has 18 samples of 1000 rowsfor activity: 6\n",
      "File dataset_1.txt has 52 samples of 1000 rowsfor activity: 7\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 8\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_1.txt has 25 samples of 1000 rowsfor activity: 10\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 13\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_2.txt has 30 samples of 1000 rowsfor activity: 6\n",
      "File dataset_2.txt has 99 samples of 1000 rowsfor activity: 7\n",
      "File dataset_2.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_2.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_2.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_3.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_3.txt has 55 samples of 1000 rowsfor activity: 7\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_3.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 13\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_4.txt has 13 samples of 1000 rowsfor activity: 5\n",
      "File dataset_4.txt has 20 samples of 1000 rowsfor activity: 6\n",
      "File dataset_4.txt has 50 samples of 1000 rowsfor activity: 7\n",
      "File dataset_4.txt has 10 samples of 1000 rowsfor activity: 8\n",
      "File dataset_4.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_4.txt has 26 samples of 1000 rowsfor activity: 11\n",
      "File dataset_4.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_4.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "attempted to create TestingData7Participants.csv ... check if the file was created!\n",
      "1018 data rows should be in the output file\n"
     ]
    }
   ],
   "source": [
    "#Challenge 2:\n",
    "#you need to create a training data set based on dataset_1,2,3,4,5,6,7,8,9 and 10\n",
    "#the testing dataset name is called week12_training_data_10Participants.csv\n",
    "\n",
    "filenamesToUseForTrainingData = []\n",
    "datasetFileNumbers = [1,2,3,4]\n",
    "\n",
    "print(\"Dataset files being used for testing data\"+ str(datasetFileNumbers))\n",
    "#\n",
    "# Start: create testing data\n",
    "#\n",
    "for number in datasetFileNumbers:\n",
    "    filenamesToUseForTrainingData.append(\"dataset_\" + str(number)+\".txt\")\n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "createTrainingDataFromFiles(filenamesToUseForTrainingData, 'TestingData7Participants.csv')\n",
    "\n",
    "\n",
    "#\n",
    "# End: create training data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrainingAndEvaluation(traingFileName, testingFileName):\n",
    "    \"\"\"\n",
    "    This function creates the confusion matrix and says the accuracy rate \n",
    "    \"\"\"\n",
    "    dfTraining = pd.read_csv(traingFileName, header = None)\n",
    "    dfTesting = pd.read_csv(testingFileName, header = None)\n",
    "\n",
    "    trainingLabels = dfTraining[9].values\n",
    "    # Labels should start from 0 in sklearn\n",
    "    trainingLabels = trainingLabels - 1\n",
    "    dfTraining = dfTraining.drop([9], axis = 1)\n",
    "    trainingFeatures = dfTraining.values\n",
    "\n",
    "    testingLabels = dfTesting[9].values\n",
    "    testingLabels = testingLabels - 1\n",
    "    dfTesting = dfTesting.drop([9], axis = 1)\n",
    "    testingFeatures = dfTesting.values\n",
    "\n",
    "    # Feature normalization for improving the performance of machine learning models. In this example code, \n",
    "    # StandardScaler is used to scale original feature to be centered around zero. You could try other normalization methods.\n",
    "    scaler = preprocessing.StandardScaler().fit(trainingFeatures)\n",
    "    trainingFeatures = scaler.transform(trainingFeatures)\n",
    "    testingFeatures = scaler.transform(testingFeatures)\n",
    "\n",
    "    # Build KNN classifier, in this example code\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    knn.fit(trainingFeatures, trainingLabels)\n",
    "\n",
    "    # Evaluation. when we train a machine learning model on training set, we should evaluate its performance on testing set.\n",
    "    # We could evaluate the model by different metrics. Firstly, we could calculate the classification accuracy. In this example\n",
    "    # code, when n_neighbors is set to 4, the accuracy achieves 0.757.\n",
    "    predictedLabels = knn.predict(testingFeatures)\n",
    "    print('Accuracy: ', accuracy_score(testingLabels, predictedLabels))\n",
    "    # We could use confusion matrix to view the classification for each activity.\n",
    "    print(confusion_matrix(testingLabels, predictedLabels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7200392927308448\n",
      "[[ 41   0   0   1   0   0   0   0   0   0   3   3   0]\n",
      " [ 12  35   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0  45   0   0   0   2   0   0   0   0   0   0]\n",
      " [  2   0   0  81   1   6   2   0   0   0   1   3   0]\n",
      " [  0   0   1  14  21   8   3   0   0   0   2   0   0]\n",
      " [  2   2   4  16  13  32  11   0   0   0   1   1   1]\n",
      " [  1   0   1   2   0   2 236  10   4   0   0   0   0]\n",
      " [  0   0   0   0   0   1  19  13   1   0   0   0   0]\n",
      " [  0   0   0   2   0   2  16   1   7   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  97   0   0   0]\n",
      " [ 13   0   0   0   0   0   0   0   0   0  32  54   0]\n",
      " [  8   1   0   9   0   0   0   0   0   0  21  59   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  34]]\n"
     ]
    }
   ],
   "source": [
    "#Challenge 3\n",
    "#use created 10 participants training data and 9 participants testing data to train and test the KNN model\n",
    "#And interpret the results\n",
    "\n",
    "# Begining your code here\n",
    "modelTrainingAndEvaluation('TrainingData12Participants.csv', 'TestingData7Participants.csv')\n",
    "\n",
    "# End\n",
    "    \n",
    "    \n",
    "# Activity values: 1 – Sitting, 2 – Lying down, 3 – Standing, 4 – Washing Dishes, 5 – Vacuuming, \n",
    "# 6 – Sweeping, 7 – Walking outside, 8 – Ascending stairs, 9 – Descending stairs, 10 – Treadmill running, 11 – Bicycling, 12 – Bicycling (more intense), 13 – Rope Jumping.\n",
    "\n",
    "\n",
    "# Can you interpret the confusion matrix below?\n",
    "# what are the rows and what are the columns?\n",
    "# which activity the machine learning predicts best?\n",
    "\n",
    "#write your answers below for tutors to check\n",
    "#Rows are for true labels\n",
    "#Columns are for predicted labels \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files being used for testing data[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_1.txt has 18 samples of 1000 rowsfor activity: 6\n",
      "File dataset_1.txt has 52 samples of 1000 rowsfor activity: 7\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 8\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_1.txt has 25 samples of 1000 rowsfor activity: 10\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 13\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_3.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_3.txt has 55 samples of 1000 rowsfor activity: 7\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_3.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 13\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_5.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_5.txt has 13 samples of 1000 rowsfor activity: 5\n",
      "File dataset_5.txt has 19 samples of 1000 rowsfor activity: 6\n",
      "File dataset_5.txt has 66 samples of 1000 rowsfor activity: 7\n",
      "File dataset_5.txt has 6 samples of 1000 rowsfor activity: 8\n",
      "File dataset_5.txt has 5 samples of 1000 rowsfor activity: 9\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_5.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_5.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_7.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_7.txt has 13 samples of 1000 rowsfor activity: 6\n",
      "File dataset_7.txt has 78 samples of 1000 rowsfor activity: 7\n",
      "File dataset_7.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_7.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_7.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_7.txt has 4 samples of 1000 rowsfor activity: 13\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_9.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 6\n",
      "File dataset_9.txt has 53 samples of 1000 rowsfor activity: 7\n",
      "File dataset_9.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_9.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_9.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_9.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_9.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_9.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_11.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_11.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_11.txt has 17 samples of 1000 rowsfor activity: 6\n",
      "File dataset_11.txt has 55 samples of 1000 rowsfor activity: 7\n",
      "File dataset_11.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_11.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_11.txt has 23 samples of 1000 rowsfor activity: 10\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_11.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_11.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_13.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_13.txt has 16 samples of 1000 rowsfor activity: 6\n",
      "File dataset_13.txt has 59 samples of 1000 rowsfor activity: 7\n",
      "File dataset_13.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_13.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_13.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_13.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_13.txt has 5 samples of 1000 rowsfor activity: 13\n",
      "File dataset_15.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_15.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_15.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_15.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_15.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_15.txt has 62 samples of 1000 rowsfor activity: 7\n",
      "File dataset_15.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_15.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_15.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_15.txt has 23 samples of 1000 rowsfor activity: 12\n",
      "File dataset_15.txt has 1 samples of 1000 rowsfor activity: 13\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_17.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_17.txt has 27 samples of 1000 rowsfor activity: 6\n",
      "File dataset_17.txt has 44 samples of 1000 rowsfor activity: 7\n",
      "File dataset_17.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_17.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_17.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_17.txt has 27 samples of 1000 rowsfor activity: 12\n",
      "File dataset_17.txt has 0 samples of 1000 rowsfor activity: 13\n",
      "File dataset_19.txt has 11 samples of 1000 rowsfor activity: 1\n",
      "File dataset_19.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_19.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_19.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_19.txt has 17 samples of 1000 rowsfor activity: 6\n",
      "File dataset_19.txt has 40 samples of 1000 rowsfor activity: 7\n",
      "File dataset_19.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_19.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_19.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_19.txt has 10 samples of 1000 rowsfor activity: 13\n",
      "attempted to create week12_training_data_10Participants.csv ... check if the file was created!\n",
      "2390 data rows should be in the output file\n",
      "Dataset files being used for testing data[2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_2.txt has 30 samples of 1000 rowsfor activity: 6\n",
      "File dataset_2.txt has 99 samples of 1000 rowsfor activity: 7\n",
      "File dataset_2.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_2.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_2.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_4.txt has 13 samples of 1000 rowsfor activity: 5\n",
      "File dataset_4.txt has 20 samples of 1000 rowsfor activity: 6\n",
      "File dataset_4.txt has 50 samples of 1000 rowsfor activity: 7\n",
      "File dataset_4.txt has 10 samples of 1000 rowsfor activity: 8\n",
      "File dataset_4.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_4.txt has 26 samples of 1000 rowsfor activity: 11\n",
      "File dataset_4.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_4.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_6.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_6.txt has 14 samples of 1000 rowsfor activity: 6\n",
      "File dataset_6.txt has 32 samples of 1000 rowsfor activity: 7\n",
      "File dataset_6.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_6.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_6.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_6.txt has 23 samples of 1000 rowsfor activity: 12\n",
      "File dataset_6.txt has 13 samples of 1000 rowsfor activity: 13\n",
      "File dataset_8.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 2\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 3\n",
      "File dataset_8.txt has 26 samples of 1000 rowsfor activity: 4\n",
      "File dataset_8.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_8.txt has 22 samples of 1000 rowsfor activity: 6\n",
      "File dataset_8.txt has 41 samples of 1000 rowsfor activity: 7\n",
      "File dataset_8.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_8.txt has 5 samples of 1000 rowsfor activity: 9\n",
      "File dataset_8.txt has 23 samples of 1000 rowsfor activity: 10\n",
      "File dataset_8.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_8.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_8.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_10.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_10.txt has 16 samples of 1000 rowsfor activity: 6\n",
      "File dataset_10.txt has 49 samples of 1000 rowsfor activity: 7\n",
      "File dataset_10.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_10.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_10.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_10.txt has 11 samples of 1000 rowsfor activity: 13\n",
      "File dataset_12.txt has 13 samples of 1000 rowsfor activity: 1\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_12.txt has 28 samples of 1000 rowsfor activity: 4\n",
      "File dataset_12.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_12.txt has 28 samples of 1000 rowsfor activity: 6\n",
      "File dataset_12.txt has 48 samples of 1000 rowsfor activity: 7\n",
      "File dataset_12.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_12.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_12.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_12.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_14.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_14.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_14.txt has 58 samples of 1000 rowsfor activity: 7\n",
      "File dataset_14.txt has 8 samples of 1000 rowsfor activity: 8\n",
      "File dataset_14.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_14.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_14.txt has 5 samples of 1000 rowsfor activity: 13\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_16.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_16.txt has 32 samples of 1000 rowsfor activity: 4\n",
      "File dataset_16.txt has 11 samples of 1000 rowsfor activity: 5\n",
      "File dataset_16.txt has 18 samples of 1000 rowsfor activity: 6\n",
      "File dataset_16.txt has 60 samples of 1000 rowsfor activity: 7\n",
      "File dataset_16.txt has 7 samples of 1000 rowsfor activity: 8\n",
      "File dataset_16.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_16.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_16.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_16.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_16.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_18.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_18.txt has 23 samples of 1000 rowsfor activity: 6\n",
      "File dataset_18.txt has 36 samples of 1000 rowsfor activity: 7\n",
      "File dataset_18.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_18.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_18.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_18.txt has 3 samples of 1000 rowsfor activity: 13\n",
      "attempted to create week12_testing_data_9Participants.csv ... check if the file was created!\n",
      "2175 data rows should be in the output file\n"
     ]
    }
   ],
   "source": [
    "# Challenge 4\n",
    "# Let's try another way for training and testing \n",
    "# Hint: training data and testing data shall not overlap \n",
    "# Solution: use odd number for training, use even number for testing\n",
    "# odd number means dataset_1, 3,5.....19, even number means dataset_2,4,6....18\n",
    "\n",
    "\n",
    "#begin your code here\n",
    "filenamesToUseForTrainingData = []\n",
    "datasetFileNumbers = [1,3,5,7,9,11,13,15,17,19]\n",
    "\n",
    "print(\"Dataset files being used for testing data\"+ str(datasetFileNumbers))\n",
    "#\n",
    "# Start: create testing data\n",
    "#\n",
    "for number in datasetFileNumbers:\n",
    "    filenamesToUseForTrainingData.append(\"dataset_\" + str(number)+\".txt\")\n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "createTrainingDataFromFiles(filenamesToUseForTrainingData, 'TrainingData12Participants.csv')\n",
    "\n",
    "filenamesToUseForTestingData = []\n",
    "datasetFileNumbers = [2,4,6,8,10,12,14,16,18]\n",
    "\n",
    "print(\"Dataset files being used for testing data\"+ str(datasetFileNumbers))\n",
    "#\n",
    "# Start: create testing data\n",
    "#\n",
    "for number in datasetFileNumbers:\n",
    "    filenamesToUseForTestingData.append(\"dataset_\" + str(number)+\".txt\")\n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "createTestingDataFromFiles(filenamesToUseForTestingData, 'TestingData7Participants.csv')\n",
    "\n",
    "\n",
    "#end code\n",
    "\n",
    "# Activity values: 1 – Sitting, 2 – Lying down, 3 – Standing, 4 – Washing Dishes, 5 – Vacuuming, \n",
    "# 6 – Sweeping, 7 – Walking outside, 8 – Ascending stairs, 9 – Descending stairs, 10 – Treadmill running, 11 – Bicycling, 12 – Bicycling (more intense), 13 – Rope Jumping.\n",
    "\n",
    "\n",
    "# Can you interpret the confusion matrix above?\n",
    "# what are the rows and what are the columns?\n",
    "# which activity the machine learning predicts best?\n",
    "\n",
    "# Has the result been improved?  why?\n",
    "\n",
    "# How to improve?\n",
    "\n",
    "#write your answers below for tutors to check\n",
    "#Rows are for true labels\n",
    "#Columns are for predicted labels \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.608735632183908\n",
      "[[ 57  23   4   3   0   2   2   0   0   0  14   4   0]\n",
      " [  5 102   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  4   0  63   9   4   4   6   1   0   0   6  10   0]\n",
      " [ 10   0   3 182   6   7  11   2   5   0   2   2   0]\n",
      " [  0   0   3  31  45  18   3   5   0   0   2   0   0]\n",
      " [  3   3   8  53  21  71  19   5   0   1   1   1   0]\n",
      " [  1   0   3  13  17  22 351  49  16   1   0   0   0]\n",
      " [  0   0   1   0   1   3  52  11   7   0   0   0   1]\n",
      " [  0   0   2   3   2   8  20   7  20   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   1 193   0   0  20]\n",
      " [ 35   2   2  13   2   4   6   6   1   0 110  40   0]\n",
      " [ 34   1  21  10   0   6   4   0   0   0  87  54   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  65]]\n"
     ]
    }
   ],
   "source": [
    "modelTrainingAndEvaluation('TrainingData12Participants.csv', 'TestingData7Participants.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare your submission\n",
    "\n",
    "# 1. prepare your code submission. if one file, name it historyCode(.ipynb), if multiple file, zip them and named it \n",
    "# historyCode.zip \n",
    "\n",
    "# 2. rename this python code file as main(.ipynb)\n",
    "\n",
    "# 3. Take a screenshot of your github commit history, name it githubCommit(.png/jpg/gif)\n",
    "\n",
    "# 4. Take a screenshot of your Agile task assignment (in Trello or whatever tools you used), name it agileTask.(.png/jpg/gif)\n",
    "\n",
    "# 5. Submit your workbook as myWorkBook.pdf (if you don't record your logbook electronically, you need to take photos of your handwritten ones and create a PDF from these photos). \n",
    "\n",
    "# 6. call the tutor to check whether your submission files are correct and if correct, submit now (all the files mentioned above).\n",
    "\n",
    "# 7. Ask the tutor what is your group's order in week 13's demo session\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
