{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is in this file:\n",
    "# Cell 2: the imports that would need to be included\n",
    "# Cell 3: a fuction provided for you (you may want to include it as a function in your code too in a fresh cell)\n",
    "# Cell 4: Challenge to create training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 6,
>>>>>>> a44b1b471c0956901d71be853249f80f4e135ce6
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed for \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "# you don't need to copy the code above... just make sure it already exists somewhere in your grou project file.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 7,
>>>>>>> a44b1b471c0956901d71be853249f80f4e135ce6
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pseudocode for the function...\n",
    "\n",
    "def create_training_data_from(list of file names, file to output to):\n",
    "\tCreate an empty training_set numpy array\n",
    "\n",
    "    for each file in the list of file names\n",
    "        import the data from the file\n",
    "\n",
    "        For each activity in the imported data:\n",
    "            get activity data into activity_data variable\n",
    "            smooth over the data (remove noise)\n",
    "            For each group of 1000 rows in that activity_data:\n",
    "                Get the minimum, maximum and average of the values for 3 axis of the wrist accelerometer\n",
    "                Store them in a temp_array\n",
    "                Add the activity number to the end of the temp_array\n",
    "\t\t\tAdd that temp_array to the end of the training_set\n",
    "\t\n",
    "\tTake the data in traning_set and save it to a file specified by the output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_training_data_from_files(list_of_filenames, output_filename):\n",
    "\n",
    "    #create the empty training set where we are going to add our \"features\"\n",
    "    training_set = np.empty(shape=(0, 10))\n",
    "    \n",
    "    for dataset_file in list_of_filenames:\n",
    "\n",
    "        #import the file contents into a panadas data frame\n",
    "        imported_data = pd.read_csv(dataset_file, sep=',', header=None)\n",
    "\n",
    "        #generate \"features\" for each activitiy\n",
    "        for activityNumber in range(1,14):\n",
    "            \n",
    "            #get all data relating to that activity and convert to a numpy ndarray\n",
    "            activity_data = imported_data[imported_data[24] == activityNumber].values\n",
    "\n",
    "            #smooth over the data for columns 0, 1, 2, ...23 (not column 24)\n",
    "            b, a = signal.butter(4, 0.04, 'low', analog=False)\n",
    "            for j in range(24):\n",
    "                activity_data[:, j] = signal.lfilter(b, a, activity_data[:, j])\n",
    "            \n",
    "            #how many full rows of 1000 are there for this activity data?\n",
    "            number_of_training_samples = int( len(activity_data)/1000 )\n",
    "            print(  \"File \" + dataset_file +\n",
    "                    \" has \" + str(number_of_training_samples) + \" samples of 1000 rows\"+\n",
    "                    \"for activity: \" + str(activityNumber))\n",
    "            \n",
    "            #for each sample of 1000 rows... scan the data and add the scan results to training_set\n",
    "            for sample_number in range(number_of_training_samples):\n",
    "                #sample data (get the next 1000 rows and all the columns)\n",
    "                sample_data = activity_data[ \n",
    "                                1000 * sample_number : 1000 * (sample_number + 1) , \n",
    "                                :\n",
    "                            ]\n",
    "                #we are about to build up a feature_sample that will have 10 columns\n",
    "                feature_sample = []\n",
    "                #sample from file 4 in week 7 prac\n",
    "                for i in range(3):\n",
    "                    feature_sample.append(np.min(sample_data[:, i]))\n",
    "                    feature_sample.append(np.max(sample_data[:, i]))\n",
    "                    feature_sample.append(np.mean(sample_data[:, i]))\n",
    "                # add the activtiy number (The last column from the row of data)\n",
    "                feature_sample.append(int(sample_data[0, -1])) \n",
    "                #make it in to an ndarray so it can be added to training data\n",
    "                feature_sample = np.array([feature_sample]) \n",
    "                training_set = np.concatenate((training_set, feature_sample), axis=0)\n",
    "            \n",
    "    #now save all this training data into a file to be used at a later date\n",
    "    df_training = pd.DataFrame(training_set)\n",
    "    df_training.to_csv(output_filename, index=None, header=None)\n",
    "    print('attempted to create '+ output_filename +' ... check if the file was created!')\n",
    "    print(str(len(training_set)) + \" data rows should be in the output file\")\n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset files being used for training data :[1, 2, 3, 4]\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_1.txt has 18 samples of 1000 rowsfor activity: 6\n",
      "File dataset_1.txt has 52 samples of 1000 rowsfor activity: 7\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 8\n",
      "File dataset_1.txt has 6 samples of 1000 rowsfor activity: 9\n",
      "File dataset_1.txt has 25 samples of 1000 rowsfor activity: 10\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_1.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_1.txt has 12 samples of 1000 rowsfor activity: 13\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_2.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_2.txt has 30 samples of 1000 rowsfor activity: 6\n",
      "File dataset_2.txt has 99 samples of 1000 rowsfor activity: 7\n",
      "File dataset_2.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_2.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_2.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 11\n",
      "File dataset_2.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_2.txt has 6 samples of 1000 rowsfor activity: 13\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_3.txt has 12 samples of 1000 rowsfor activity: 5\n",
      "File dataset_3.txt has 15 samples of 1000 rowsfor activity: 6\n",
      "File dataset_3.txt has 55 samples of 1000 rowsfor activity: 7\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 8\n",
      "File dataset_3.txt has 7 samples of 1000 rowsfor activity: 9\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 11\n",
      "File dataset_3.txt has 24 samples of 1000 rowsfor activity: 12\n",
      "File dataset_3.txt has 9 samples of 1000 rowsfor activity: 13\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 1\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 2\n",
      "File dataset_4.txt has 12 samples of 1000 rowsfor activity: 3\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 4\n",
      "File dataset_4.txt has 13 samples of 1000 rowsfor activity: 5\n",
      "File dataset_4.txt has 20 samples of 1000 rowsfor activity: 6\n",
      "File dataset_4.txt has 50 samples of 1000 rowsfor activity: 7\n",
      "File dataset_4.txt has 10 samples of 1000 rowsfor activity: 8\n",
      "File dataset_4.txt has 8 samples of 1000 rowsfor activity: 9\n",
      "File dataset_4.txt has 24 samples of 1000 rowsfor activity: 10\n",
      "File dataset_4.txt has 26 samples of 1000 rowsfor activity: 11\n",
      "File dataset_4.txt has 25 samples of 1000 rowsfor activity: 12\n",
      "File dataset_4.txt has 7 samples of 1000 rowsfor activity: 13\n",
      "attempted to create week11_training_data_4Participants.csv ... check if the file was created!\n",
      "1018 data rows should be in the output file\n"
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File d does not exist: 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9c90067716c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#the training dataset name is called week11_training_data_4Participants.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcreate_training_data_from_files\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'dataset_1.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'week11_training_data_4Participants.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-7b4726023220>\u001b[0m in \u001b[0;36mcreate_training_data_from_files\u001b[0;34m(list_of_filenames, output_filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#import the file contents into a panadas data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mimported_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#generate \"features\" for each activitiy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File d does not exist: 'd'"
>>>>>>> a44b1b471c0956901d71be853249f80f4e135ce6
     ]
    }
   ],
   "source": [
    "#Challenge:\n",
    "#you need to create a training data set based on dataset_1, dataset_2, dataset_3, and dataset_4\n",
    "#the training dataset name is called week11_training_data_4Participants.csv\n",
    "\n",
<<<<<<< HEAD
    "filnamesToUseForTrainingData = []\n",
    "datasetFileNumbers = [ 1, 2, 3, 4]\n",
    "\n",
    "print(\"Dataset files being used for training data :\" + str(datasetFileNumbers))\n",
=======
    "create_training_data_from_files ('dataset_1.txt','week11_training_data_4Participants.csv')\n",
>>>>>>> a44b1b471c0956901d71be853249f80f4e135ce6
    "\n",
    "#\n",
    "# Start: create training data\n",
    "#\n",
    "for numbers in datasetFileNumbers:\n",
    "    filnamesToUseForTrainingData.append(\"dataset_\"+str(numbers)+\".txt\")\n",
    "    \n",
    "create_training_data_from_files( filnamesToUseForTrainingData, \"week11_training_data_4Participants.csv\")\n",
    "    \n",
    "\n",
    "#provide your code below for tutors to check\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# End: create training data\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
